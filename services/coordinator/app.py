import os
import json
import logging
import asyncio
import aiohttp
import websockets
import time
import re
import uuid
from enum import Enum
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File, Response
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import requests
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv
load_dotenv() 

# å¯¼å…¥ç³»ç»Ÿé…ç½®
from system_config import (
    ENVIRONMENTAL_DATA,
    get_comprehensive_system_prompt,
    extract_iot_commands_enhanced,
    determine_expression_enhanced,
    get_scene_actions,
    update_sensor_data,
    get_all_sensors_data,
    SCENE_MODES
)
from llm_iot_extractor import LLMIoTExtractor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# API Mode Enum
class APIMode(Enum):
    LOCAL_OLLAMA = "local"
    REMOTE_API = "api"

# Default configuration (will be overridden by config file if exists)
DEFAULT_API_MODE = "api"  # Default mode if no config file exists
DEFAULT_LOCAL_MODEL = "llama3.2:3b"
DEFAULT_REMOTE_MODEL = "llama3.2:3b"

# Local Ollama configuration
OLLAMA_HOST = "ollama"
OLLAMA_PORT = "11434"
OLLAMA_SCHEME = "http"

# Remote API configuration (protected by .env)
REMOTE_API_URL = "https://chat.cetools.org/api/chat/completions"
REMOTE_API_KEY = os.getenv("REMOTE_API_KEY", "")  # Must be set in .env

# Other services configuration
STT_HOST = "stt-service"
STT_PORT = "8000"
TTS_HOST = "tts-service"
TTS_PORT = "8001"
TTS_VOICE = "en-US-AriaNeural"
IOT_HOST = "iot-control"
IOT_PORT = "8002"

# Early configuration loading function
def load_early_config():
    """Load configuration from file early in the startup process"""
    config_file = "/app/config/model_preferences.json"
    
    # Default values
    api_mode = DEFAULT_API_MODE
    default_model = DEFAULT_REMOTE_MODEL if DEFAULT_API_MODE == "api" else DEFAULT_LOCAL_MODEL
    
    try:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                preferences = json.load(f)
            
            # Get mode from config file
            saved_mode = preferences.get("current_mode")
            if saved_mode and saved_mode in ["local", "api"]:
                # Check if API key is available for API mode
                if saved_mode == "api" and not REMOTE_API_KEY:
                    logger.warning("âš ï¸ Config requests API mode but REMOTE_API_KEY not set, falling back to default")
                    api_mode = DEFAULT_API_MODE
                else:
                    api_mode = saved_mode
                    logger.info(f"ğŸ“‚ Loaded API mode from config: {api_mode}")
                    
                    # Also load the saved model for this mode
                    saved_model = preferences.get("current_model")
                    if saved_model:
                        default_model = saved_model
                        logger.info(f"ğŸ“‚ Loaded default model from config: {default_model}")
        else:
            logger.info(f"ğŸ“„ No config file found, using default API mode: {api_mode}")
            
    except Exception as e:
        logger.error(f"âŒ Error loading early config: {str(e)}, using defaults")
    
    return api_mode, default_model

# Load configuration early
API_MODE, INITIAL_MODEL = load_early_config()
OLLAMA_MODEL = INITIAL_MODEL if API_MODE == "local" else DEFAULT_LOCAL_MODEL
REMOTE_API_MODEL = INITIAL_MODEL if API_MODE == "api" else DEFAULT_REMOTE_MODEL

# Build endpoint based on mode
if API_MODE == "api":
    OLLAMA_ENDPOINT = REMOTE_API_URL
    if not REMOTE_API_KEY:
        logger.warning("âš ï¸ REMOTE_API_KEY not set in environment. API mode may not work properly.")
else:
    if OLLAMA_PORT:
        OLLAMA_ENDPOINT = f"{OLLAMA_SCHEME}://{OLLAMA_HOST}:{OLLAMA_PORT}"
    else:
        OLLAMA_ENDPOINT = f"{OLLAMA_SCHEME}://{OLLAMA_HOST}"

logger.info(f"ğŸš€ Starting in {API_MODE} mode with endpoint: {OLLAMA_ENDPOINT}")

# Enhanced ModelManager with API mode support
class EnhancedModelManager:
    def __init__(self):
        # Use the early-loaded configuration
        self.current_mode = APIMode(API_MODE)
        self.current_model = REMOTE_API_MODEL if self.current_mode == APIMode.REMOTE_API else OLLAMA_MODEL
        self.available_models = []
        self.model_info = {}
        self.last_model_check = 0
        self.model_check_interval = 30  # 30ç§’æ£€æŸ¥ä¸€æ¬¡å¯ç”¨æ¨¡å‹
        
        # API headers for remote mode
        self.api_headers = {}
        if self.current_mode == APIMode.REMOTE_API and REMOTE_API_KEY:
            self.api_headers = {
                "Authorization": f"Bearer {REMOTE_API_KEY}",
                "Content-Type": "application/json"
            }
        
        # é…ç½®æ–‡ä»¶è·¯å¾„
        self.config_dir = "/app/config"
        self.config_file = os.path.join(self.config_dir, "model_preferences.json")
        
        # éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®æ˜¯å¦ä¸å½“å‰è®¾ç½®ä¸€è‡´
        self._validate_config_consistency()
    
    def _validate_config_consistency(self):
        """éªŒè¯é…ç½®æ–‡ä»¶ä¸å½“å‰è¿è¡ŒçŠ¶æ€çš„ä¸€è‡´æ€§"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    preferences = json.load(f)
                
                saved_mode = preferences.get("current_mode")
                saved_model = preferences.get("current_model")
                
                # å¦‚æœé…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å¼ä¸å½“å‰æ¨¡å¼ä¸ä¸€è‡´ï¼Œæ›´æ–°é…ç½®æ–‡ä»¶
                if saved_mode != self.current_mode.value:
                    logger.info(f"ğŸ”„ Updating config file mode from {saved_mode} to {self.current_mode.value}")
                    self._save_preferences_to_file()
                
                # ç¡®ä¿æ¨¡å‹è®¾ç½®æ­£ç¡®
                if saved_model and saved_model != self.current_model:
                    logger.info(f"ğŸ”„ Model mismatch detected, using: {self.current_model}")
        
        except Exception as e:
            logger.error(f"âŒ Error validating config consistency: {str(e)}")
    
    def get_current_mode(self):
        return self.current_mode.value
    
    def switch_mode(self, mode: str):
        """Switch between local and API mode"""
        if mode not in ["local", "api"]:
            raise ValueError("Mode must be 'local' or 'api'")
        
        # Check API key for API mode
        if mode == "api" and not REMOTE_API_KEY:
            raise ValueError("Cannot switch to API mode: REMOTE_API_KEY not configured")
        
        self.current_mode = APIMode.LOCAL_OLLAMA if mode == "local" else APIMode.REMOTE_API
        
        # Update model and headers based on mode
        if self.current_mode == APIMode.REMOTE_API:
            self.current_model = REMOTE_API_MODEL
            self.api_headers = {
                "Authorization": f"Bearer {REMOTE_API_KEY}",
                "Content-Type": "application/json"
            }
        else:
            self.current_model = OLLAMA_MODEL
            self.api_headers = {}

        # æ›´æ–°å…¨å±€ llm_extractor çš„é…ç½®ï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
        global llm_extractor
        if llm_extractor is not None:
            llm_extractor.model_name = self.current_model
            llm_extractor.api_headers = self.api_headers
            llm_extractor.ollama_endpoint = OLLAMA_ENDPOINT
            logger.info(f"âœ… Updated LLM extractor for {mode} mode")
        
        # Save mode preference
        self._save_preferences_to_file()
        
        logger.info(f"âœ… Switched to {mode} mode with model {self.current_model}")
        return True
    
    def _ensure_config_directory(self):
        """ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨"""
        try:
            os.makedirs(self.config_dir, exist_ok=True)
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to create config directory: {str(e)}")
            return False
    
    def _save_preferences_to_file(self):
        """è‡ªåŠ¨ä¿å­˜æ¨¡å‹åå¥½è®¾ç½®åˆ°é…ç½®æ–‡ä»¶"""
        try:
            if not self._ensure_config_directory():
                return False
            
            # æ„å»ºé…ç½®æ•°æ®
            preferences = {
                "current_mode": self.current_mode.value,
                "current_model": self.current_model,
                "available_models": [
                    {
                        "name": model_name,
                        "info": self.model_info.get(model_name, {})
                    }
                    for model_name in self.available_models
                ],
                "last_updated": time.time(),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(preferences, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… Model preferences saved to {self.config_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to save preferences: {str(e)}")
            return False
    
    async def get_available_models(self, force_refresh=False):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ - æ”¯æŒä¸¤ç§æ¨¡å¼"""
        current_time = time.time()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°æ¨¡å‹åˆ—è¡¨
        if not force_refresh and self.available_models and \
           (current_time - self.last_model_check) < self.model_check_interval:
            return self.available_models
        
        try:
            if self.current_mode == APIMode.REMOTE_API:
                # APIæ¨¡å¼ï¼šè¿”å›é¢„å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨
                self.available_models = ["llama3.2:3b", "llama3.1:70b", "gemma2:latest", "gemma3:latest","llama3:8b"]
                
                # è®¾ç½®æ¨¡å‹ä¿¡æ¯
                for model in self.available_models:
                    self.model_info[model] = {
                        "name": model,
                        "modified_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                        "size": 0,  # APIæ¨¡å¼ä¸æ˜¾ç¤ºå¤§å°
                        "details": {"format": "api", "family": model.split(":")[0]}
                    }
                
                logger.info(f"ğŸ“‹ Available API models: {self.available_models}")
                
            else:
                # æœ¬åœ°æ¨¡å¼ï¼šä»Ollamaè·å–æ¨¡å‹åˆ—è¡¨
                response = requests.get(f"{OLLAMA_ENDPOINT}/api/tags", timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    models = data.get("models", [])
                    
                    self.available_models = []
                    for model in models:
                        model_name = model.get("name", "")
                        if model_name:
                            self.available_models.append(model_name)
                            self.model_info[model_name] = {
                                "name": model_name,
                                "modified_at": model.get("modified_at", ""),
                                "size": model.get("size", 0),
                                "details": model.get("details", {})
                            }
                    
                    logger.info(f"ğŸ“‹ Found {len(self.available_models)} local models")
                else:
                    logger.error(f"âŒ Failed to get models from Ollama: HTTP {response.status_code}")
                    
        except Exception as e:
            logger.error(f"âŒ Error getting available models: {str(e)}")
        
        self.last_model_check = current_time
        self._save_preferences_to_file()
        
        return self.available_models
    
    def get_current_model(self):
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
        return self.current_model
    
    def set_current_model(self, model_name):
        """è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
        self.current_model = model_name
        self._save_preferences_to_file()
        logger.info(f"âœ… Current model set to: {model_name}")
        return True
    
    def get_model_info(self, model_name):
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        return self.model_info.get(model_name, {})
    
    def get_config_status(self):
        """è·å–é…ç½®æ–‡ä»¶çŠ¶æ€"""
        config_exists = os.path.exists(self.config_file)
        config_info = {
            "config_exists": config_exists,
            "config_file_path": self.config_file,
            "config_dir": self.config_dir,
            "config_size_bytes": 0,
            "config_modified": None,
            "total_available_models": len(self.available_models),
            "current_model": self.current_model,
            "current_mode": self.current_mode.value
        }
        
        if config_exists:
            try:
                stat = os.stat(self.config_file)
                config_info["config_size_bytes"] = stat.st_size
                config_info["config_modified"] = time.strftime(
                    "%Y-%m-%d %H:%M:%S", 
                    time.localtime(stat.st_mtime)
                )
            except Exception as e:
                logger.error(f"Error getting config file stats: {str(e)}")
        
        return config_info
    
    def reset_preferences(self):
        """é‡ç½®åå¥½è®¾ç½®åˆ°é»˜è®¤å€¼"""
        try:
            # åˆ é™¤é…ç½®æ–‡ä»¶
            if os.path.exists(self.config_file):
                os.remove(self.config_file)
                logger.info(f"ğŸ—‘ï¸ Removed config file: {self.config_file}")
            
            # é‡ç½®ä¸ºé»˜è®¤å€¼
            self.current_mode = APIMode(DEFAULT_API_MODE)
            self.current_model = DEFAULT_REMOTE_MODEL if self.current_mode == APIMode.REMOTE_API else DEFAULT_LOCAL_MODEL
            self.available_models = []
            self.model_info = {}
            
            logger.info("âœ… Preferences reset to defaults")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to reset preferences: {str(e)}")
            return False

# Initialize model manager
model_manager = EnhancedModelManager()

# åœ¨ EnhancedModelManager åˆå§‹åŒ–åæ·»åŠ 
llm_extractor = LLMIoTExtractor(
    ollama_endpoint=OLLAMA_ENDPOINT,
    model_name=model_manager.current_model,
    api_headers=model_manager.api_headers
)

# Global variables
startup_time = time.time()
environmental_data = ENVIRONMENTAL_DATA  # ä½¿ç”¨å¯¼å…¥çš„æ•°æ®

# Create FastAPI application
app = FastAPI(title="AI Voice Assistant Coordinator Service - Enhanced with API Mode")

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Connected clients and devices
connected_clients = {}
registered_devices = {}
user_contexts = {}

# Request models
class AudioRequest(BaseModel):
    audio_path: str

# 1. ä¿®æ”¹ TextRequest æ¨¡å‹ï¼Œæ·»åŠ  device_id å­—æ®µ
class TextRequest(BaseModel):
    text: str
    device_id: Optional[str] = None  # æ–°å¢å­—æ®µ
    source: Optional[str] = None     # æ–°å¢å­—æ®µ
    location: Optional[str] = "living_room"
    timestamp: Optional[float] = None

class ModelSwitchRequest(BaseModel):
    model_name: str

class ModelPullRequest(BaseModel):
    model_name: str

class ContextualRequest(BaseModel):
    text: str
    user_context: Optional[Dict[str, Any]] = None
    location: str = "living_room"
    device_id: Optional[str] = None

class SceneRequest(BaseModel):
    scene_name: str
    location: Optional[str] = None

class DeviceRegistration(BaseModel):
    device_info: Dict[str, Any]
    capabilities: List[str]

class ConfigResetRequest(BaseModel):
    confirm: bool = False

class ConfigExportRequest(BaseModel):
    export_path: Optional[str] = None

class ModeSwitchRequest(BaseModel):
    mode: str  # "local" or "api"

async def process_with_llm(text_input: str, context: Dict = None, location: str = "living_room"):
    """Process text with LLM based on current mode"""
    try:
        # è·å–æ‰€æœ‰æˆ¿é—´çš„å®æ—¶çŠ¶æ€å¹¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
        enhanced_context = context or {}
        
        # å°è¯•ä»IoTæœåŠ¡è·å–æ‰€æœ‰ä¼ æ„Ÿå™¨çš„æœ€æ–°æ•°æ®
        try:
            sensors_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/sensors", timeout=2)
            if sensors_response.status_code == 200:
                all_sensors = sensors_response.json().get("sensors", {})
                # æ›´æ–°æœ¬åœ°çš„ENVIRONMENTAL_DATA
                ENVIRONMENTAL_DATA["sensors"].update(all_sensors)
                enhanced_context["real_time_sensors"] = all_sensors
        except Exception as e:
            logger.warning(f"Could not fetch real-time sensor data: {e}")
        
        # è·å–æ‰€æœ‰è®¾å¤‡çŠ¶æ€
        try:
            devices_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/devices", timeout=2)
            if devices_response.status_code == 200:
                all_devices = devices_response.json().get("devices", {})
                enhanced_context["device_states"] = all_devices
        except Exception as e:
            logger.warning(f"Could not fetch device states: {e}")
        
        # ç”ŸæˆåŒ…å«æ‰€æœ‰æˆ¿é—´ä¿¡æ¯çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = get_comprehensive_system_prompt(enhanced_context, location)
        
        # ç»§ç»­åŸæœ‰çš„å¤„ç†é€»è¾‘...
        if model_manager.current_mode == APIMode.REMOTE_API:
            # Use OpenAI-compatible API format
            payload = {
                "model": model_manager.current_model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text_input}
                ],
                "temperature": 0.7,
                "max_tokens": 500
            }
            
            response = requests.post(
                OLLAMA_ENDPOINT,
                json=payload,
                headers=model_manager.api_headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"]
            else:
                logger.error(f"API request failed: {response.status_code} - {response.text}")
                return "I apologize, I couldn't process your request."
                
        else:
            # Use original Ollama format
            payload = {
                "model": model_manager.current_model,
                "prompt": f"{system_prompt}\n\nUser: {text_input}\n\nAssistant:",
                "stream": False
            }
            
            response = requests.post(
                f"{OLLAMA_ENDPOINT}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json().get("response", "I apologize, I couldn't process your request.")
            else:
                logger.error(f"Ollama request failed: {response.status_code}")
                return "I apologize, I couldn't process your request."
                
    except Exception as e:
        logger.error(f"Error processing with LLM: {str(e)}")
        return "I apologize, an error occurred while processing your request."

def is_valid_action(device_type: str, action: str) -> bool:
    """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦é€‚ç”¨äºè®¾å¤‡ç±»å‹"""
    valid_actions = {
        "light": ["on", "off", "brighten", "dim"],
        "fan": ["on", "off", "speed_up", "speed_down"],
        "ac": ["on", "off", "temp_up", "temp_down"],
        "curtain": ["on", "off"]  # on=open, off=close
    }
    
    return action in valid_actions.get(device_type, [])

# 2. ä¿®æ”¹ process_text_with_enhanced_llm å‡½æ•°ç­¾å
async def process_text_with_enhanced_llm(
    text_input: str,
    user_context: str = None,
    location: str = "living_room",
    device_id: str = None  # æ–°å¢å‚æ•°
):
    """Enhanced text processing with current model and all features"""
    
    # Get current model
    current_model = model_manager.get_current_model()
    
    # 1. Generate system prompt
    system_prompt = get_comprehensive_system_prompt(user_context, location)
    
    # 2. Extract IoT commands
    # iot_commands = extract_iot_commands_enhanced(text_input, location) # Old version
    # ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹é…ç½®
    llm_extractor.model_name = model_manager.current_model
    llm_extractor.api_headers = model_manager.api_headers
    llm_extractor.ollama_endpoint = OLLAMA_ENDPOINT
    
    try:
        iot_commands = llm_extractor.extract_iot_commands_with_llm(text_input, location)
        logger.info(f"ğŸ¤– LLM extracted {len(iot_commands)} IoT commands")
    except Exception as e:
        logger.error(f"LLM extraction failed: {e}, using empty command list")
        iot_commands = []
    
    # 3. Execute IoT commands
    iot_results = []
    if iot_commands:
        try:
            iot_url = f"http://{IOT_HOST}:{IOT_PORT}/control"
            # æ­£ç¡®æ ¼å¼ï¼šå°†æ‰€æœ‰å‘½ä»¤æ”¾åœ¨commandsæ•°ç»„ä¸­
            iot_response = requests.post(iot_url, json={"commands": iot_commands})
            
            if iot_response.status_code == 200:
                iot_results = iot_response.json().get("results", [])
            else:
                iot_results = [{"error": f"IoT command failed: {iot_response.text}"}]
        except Exception as e:
            iot_results = [{"error": f"IoT service error: {str(e)}"}]
    
    # 4. Generate AI response using current model
    # åˆ›å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«IoTæ‰§è¡Œç»“æœ
    enhanced_context = {
        "iot_commands": iot_commands,  # æ‰§è¡Œçš„å‘½ä»¤
        "iot_results": iot_results,     # æ‰§è¡Œç»“æœ
    }

    # å¦‚æœæœ‰user_contextï¼Œåˆå¹¶è¿›å»
    if user_context:
        if isinstance(user_context, dict):
            enhanced_context.update(user_context)
        else:
            enhanced_context["user_info"] = user_context

    ai_response = await process_with_llm(text_input, enhanced_context, location)

    # 4.5 æ¸…ç†AIå“åº”ä¸­çš„markdownæ ‡è®°ï¼ˆæ–°å¢ï¼‰
    # å»é™¤æ‰€æœ‰çš„markdownæ ¼å¼ç¬¦å·ï¼Œè®©TTSè¯»èµ·æ¥æ›´è‡ªç„¶
    cleaned_response = ai_response
    # å»é™¤åŠ ç²—æ ‡è®°
    cleaned_response = cleaned_response.replace("**", "")
    # å»é™¤æ–œä½“æ ‡è®°
    cleaned_response = cleaned_response.replace("*", "")
    
    # 5. Determine expression/emotion
    expression = determine_expression_enhanced(text_input, cleaned_response, iot_commands)
    
    # 6. Generate TTS
    audio_url = None
    audio_path = None
    audio_id = None  # æ–°å¢å­—æ®µç”¨äºESP32
    
    try:
        # æ ¹æ®æ˜¯å¦æœ‰ device_id é€‰æ‹©ä¸åŒçš„ç«¯ç‚¹
        if device_id:
            # ESP32è®¾å¤‡ï¼šä½¿ç”¨ä¸“é—¨çš„ç«¯ç‚¹
            tts_url = f"http://{TTS_HOST}:{TTS_PORT}/esp32/synthesize"
            tts_payload = {
                "text": cleaned_response,
                "voice": TTS_VOICE,
                "format": "pcm"  # ESP32ä½¿ç”¨PCMæ ¼å¼
            }
            headers = {
                "X-Device-ID": device_id
            }
            
            logger.info(f"ğŸ”Š Requesting TTS for ESP32 device {device_id}: {cleaned_response[:50]}...")
            tts_response = requests.post(tts_url, json=tts_payload, headers=headers, timeout=10)
            
            if tts_response.status_code == 200:
                response_data = tts_response.json()
                audio_id = response_data.get("audio_id")
                logger.info(f"âœ… TTS task created for ESP32: {audio_id}")
        else:
            # Webå®¢æˆ·ç«¯ï¼šä½¿ç”¨åŸæœ‰ç«¯ç‚¹
            tts_url = f"http://{TTS_HOST}:{TTS_PORT}/synthesize"
            tts_payload = {
                "text": cleaned_response,
                "voice": TTS_VOICE,
                "format": "mp3"
            }
            
            logger.info(f"ğŸ”Š Requesting TTS for web client: {cleaned_response[:50]}...")
            tts_response = requests.post(tts_url, json=tts_payload, timeout=10)
            
            if tts_response.status_code == 200:
                audio_data = tts_response.json()
                audio_path = audio_data.get("audio_path", "")
                
                if audio_path:
                    audio_filename = audio_path.split('/')[-1]
                    audio_url = f"http://{TTS_HOST}:{TTS_PORT}/audio/{audio_filename}"
                    logger.info(f"âœ… TTS generated successfully: {audio_url}")
                else:
                    logger.warning("TTS response missing audio_path")
        
        if tts_response.status_code != 200:
            logger.error(f"TTS request failed with status {tts_response.status_code}")
            logger.error(f"TTS error response: {tts_response.text}")
            
    except requests.exceptions.Timeout:
        logger.error("TTS request timeout")
    except Exception as e:
        logger.error(f"TTS generation error: {str(e)}")
    
    # 7. ä¿å­˜é…ç½®
    config_info = model_manager.get_config_status()
    
    # æ„å»ºè¿”å›ç»“æœ
    result = {
        "input_text": text_input,
        "ai_response": cleaned_response,
        "expression": expression,
        "iot_commands": iot_commands,
        "iot_results": iot_results,
        "location": location,
        "user_context": user_context,
        "model_used": current_model,
        "model_info": model_manager.get_model_info(current_model),
        "config_file_info": {
            "config_updated": config_info["config_exists"],
            "config_file_path": config_info["config_file_path"]
        }
    }
    
    # æ ¹æ®è®¾å¤‡ç±»å‹æ·»åŠ ä¸åŒçš„éŸ³é¢‘ä¿¡æ¯
    if device_id:
        result["device_id"] = device_id
        result["audio_id"] = audio_id
        result["audio_format"] = "pcm"
    else:
        result["audio_path"] = audio_path
        result["audio_url"] = audio_url
        result["audio_format"] = "mp3"
    
    return result

# API Endpoints

@app.get("/")
async def root():
    """Root endpoint with API mode info"""
    return {
        "service": "AI Voice Assistant Coordinator - Enhanced",
        "version": "3.0",
        "mode": model_manager.get_current_mode(),
        "model": model_manager.current_model,
        "features": [
            "Voice recognition (STT)",
            "Text-to-Speech (TTS)", 
            "IoT device control",
            "Natural language processing",
            "Multi-language support",
            "API mode switching"
        ],
        "api_mode": {
            "current": model_manager.get_current_mode(),
            "available": ["local", "api"],
            "endpoint": OLLAMA_ENDPOINT if model_manager.current_mode == APIMode.REMOTE_API else f"{OLLAMA_SCHEME}://{OLLAMA_HOST}:{OLLAMA_PORT}"
        }
    }

@app.get("/health")
async def health_check():
    """Enhanced health check with mode info"""
    return {
        "status": "healthy",
        "mode": model_manager.get_current_mode(),
        "model": model_manager.current_model,
        "services": {
            "stt": "connected",
            "tts": "connected",
            "iot": "connected",
            "llm": model_manager.get_current_mode()
        },
        "uptime": time.time() - startup_time,
        "timestamp": time.time()
    }

# API Mode Management Endpoints

@app.get("/api/mode")
async def get_api_mode():
    """Get current API mode"""
    return {
        "mode": model_manager.get_current_mode(),
        "model": model_manager.current_model,
        "endpoint": OLLAMA_ENDPOINT if model_manager.current_mode == APIMode.REMOTE_API else f"{OLLAMA_SCHEME}://{OLLAMA_HOST}:{OLLAMA_PORT}",
        "available_modes": ["local", "api"],
        "api_key_configured": bool(REMOTE_API_KEY) if model_manager.current_mode == APIMode.REMOTE_API else None
    }

@app.post("/api/mode/switch")
async def switch_api_mode(request: ModeSwitchRequest):
    """Switch between local and API mode - requires service restart"""
    try:
        mode = request.mode
        if not mode:
            return JSONResponse(
                status_code=400,
                content={"error": "Mode parameter required"}
            )
        
        # Check if mode is actually changing
        current_mode = model_manager.get_current_mode()
        if mode == current_mode:
            return {
                "success": False,
                "mode": current_mode,
                "message": f"Already in {mode} mode, no restart needed"
            }
        
        success = model_manager.switch_mode(mode)
        
        if success:
            # Save configuration before restart
            logger.info(f"ğŸ”„ Mode switch successful, preparing to restart service...")
            
            # Give a response before restarting
            response_data = {
                "success": True,
                "mode": mode,
                "model": model_manager.current_model,
                "message": f"Switched to {mode} mode. Service will restart in 2 seconds...",
                "restart_required": True,
                "endpoint": REMOTE_API_URL if mode == "api" else f"{OLLAMA_SCHEME}://{OLLAMA_HOST}:{OLLAMA_PORT}"
            }
            
            # Schedule restart after response is sent
            async def delayed_restart():
                await asyncio.sleep(2)  # Give time for response to be sent
                logger.info("ğŸ”„ Restarting service to apply mode change...")
                
                # Different restart methods based on environment
                import os
                import sys
                import signal
                
                # Method 1: For Docker containers - exit and let Docker restart
                if os.environ.get('DOCKER_CONTAINER'):
                    logger.info("ğŸ³ Docker environment detected, exiting for container restart...")
                    os._exit(0)  # Exit cleanly, Docker will restart
                
                # Method 2: For standalone Python - restart the process
                else:
                    logger.info("ğŸ”„ Standalone environment, restarting Python process...")
                    try:
                        # Flush all outputs
                        sys.stdout.flush()
                        sys.stderr.flush()
                        
                        # Replace current process with new one
                        os.execv(sys.executable, ['python'] + sys.argv)
                    except Exception as e:
                        logger.error(f"Failed to restart: {e}")
                        # Fallback: just exit
                        sys.exit(0)
            
            # Start restart task in background
            asyncio.create_task(delayed_restart())
            
            return response_data
            
        else:
            return JSONResponse(
                status_code=500,
                content={"error": "Failed to switch mode"}
            )
            
    except ValueError as e:
        return JSONResponse(
            status_code=400,
            content={"error": str(e)}
        )
    except Exception as e:
        logger.error(f"Error switching mode: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Internal error: {str(e)}"}
        )

# Model Management Endpoints

@app.get("/models")
async def get_available_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ - æ”¯æŒä¸¤ç§æ¨¡å¼"""
    try:
        models = await model_manager.get_available_models(force_refresh=True)
        current_model = model_manager.get_current_model()
        config_status = model_manager.get_config_status()
        
        model_list = []
        for model_name in models:
            model_info = model_manager.get_model_info(model_name)
            model_list.append({
                "name": model_name,
                "size": model_info.get("size", 0),
                "size_mb": round(model_info.get("size", 0) / (1024 * 1024), 1) if model_info.get("size", 0) > 0 else 0,
                "modified_at": model_info.get("modified_at", ""),
                "is_current": model_name == current_model,
                "details": model_info.get("details", {})
            })
        
        return {
            "current_model": current_model,
            "current_mode": model_manager.get_current_mode(),
            "available_models": model_list,
            "total_models": len(models),
            "config_file_info": {
                "exists": config_status["config_exists"],
                "path": config_status["config_file_path"],
                "size_bytes": config_status["config_size_bytes"],
                "last_modified": config_status["config_modified"]
            }
        }
    
    except Exception as e:
        logger.error(f"Error getting models: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error getting models: {str(e)}"}
        )

@app.get("/models/current")
async def get_current_model():
    """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    return {
        "current_model": model_manager.get_current_model(),
        "mode": model_manager.get_current_mode()
    }

@app.post("/models/switch")
async def switch_model(request: ModelSwitchRequest):
    """åˆ‡æ¢å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
    try:
        logger.info(f"ğŸ”„ Attempting to switch to model: {request.model_name}")
        
        # å…ˆè·å–æœ€æ–°çš„æ¨¡å‹åˆ—è¡¨
        available_models = await model_manager.get_available_models(force_refresh=True)
        
        if request.model_name not in available_models:
            return JSONResponse(
                status_code=404,
                content={
                    "error": f"Model not found: {request.model_name}",
                    "available_models": available_models
                }
            )
        
        # è·å–åˆ‡æ¢å‰çš„çŠ¶æ€
        old_model = model_manager.get_current_model()
        config_status_before = model_manager.get_config_status()
        
        # åˆ‡æ¢æ¨¡å‹ï¼ˆä¼šè‡ªåŠ¨ä¿å­˜é…ç½®æ–‡ä»¶ï¼‰
        success = model_manager.set_current_model(request.model_name)
        
        if success:
            # è·å–åˆ‡æ¢åçš„é…ç½®çŠ¶æ€
            config_status_after = model_manager.get_config_status()
            
            # æµ‹è¯•æ–°æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
            test_result = await test_model_functionality(request.model_name)
            
            if test_result["success"]:
                # å¹¿æ’­æ¨¡å‹åˆ‡æ¢äº‹ä»¶åˆ°æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
                await broadcast_model_switch(request.model_name)
                
                logger.info(f"âœ… Successfully switched from {old_model} to {request.model_name}")
                
                return {
                    "success": True,
                    "message": f"Successfully switched to model: {request.model_name}",
                    "old_model": old_model,
                    "current_model": request.model_name,
                    "test_result": test_result,
                    "config_file_info": {
                        "config_updated": True,
                        "config_file_path": config_status_after["config_file_path"],
                        "before_switch": config_status_before,
                        "after_switch": config_status_after
                    }
                }
            else:
                # å¦‚æœæµ‹è¯•å¤±è´¥ï¼Œå›æ»šåˆ°åŸæ¨¡å‹
                model_manager.set_current_model(old_model)
                
                return JSONResponse(
                    status_code=500,
                    content={
                        "error": f"Model test failed: {test_result.get('error', 'Unknown error')}",
                        "model_name": request.model_name,
                        "reverted_to": old_model
                    }
                )
        else:
            return JSONResponse(
                status_code=500,
                content={"error": f"Failed to switch model to {request.model_name}"}
            )
            
    except Exception as e:
        logger.error(f"Error switching model: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error switching model: {str(e)}"}
        )

@app.post("/models/pull")
async def pull_model(request: ModelPullRequest):
    """ä¸‹è½½æ–°æ¨¡å‹ - ä»…æ”¯æŒæœ¬åœ°æ¨¡å¼"""
    if model_manager.current_mode == APIMode.REMOTE_API:
        return JSONResponse(
            status_code=400,
            content={"error": "Model pulling is only available in local mode"}
        )
    
    try:
        model_name = request.model_name
        logger.info(f"ğŸ“¥ Starting to pull model: {model_name}")
        
        ollama_url = f"{OLLAMA_ENDPOINT}/api/pull"
        pull_payload = {"name": model_name}
        
        response = requests.post(
            ollama_url,
            json=pull_payload,
            timeout=600  # 10 minutes for large models
        )
        
        if response.status_code == 200:
            logger.info(f"âœ… Successfully pulled model: {model_name}")
            
            # Refresh model list
            await model_manager.get_available_models(force_refresh=True)
            
            # Test the model works
            test_result = await test_model_functionality(model_name)
            
            return {
                "success": True,
                "message": f"Model {model_name} downloaded successfully",
                "model_name": model_name,
                "test_result": test_result
            }
        else:
            error_text = response.text
            logger.error(f"âŒ Model pull failed: HTTP {response.status_code}")
            
            return JSONResponse(
                status_code=response.status_code,
                content={
                    "success": False,
                    "error": f"Failed to download model: {error_text}",
                    "model_name": model_name
                }
            )
            
    except requests.exceptions.Timeout:
        logger.error(f"âŒ Timeout pulling model {model_name}")
        return JSONResponse(
            status_code=408,
            content={
                "success": False,
                "error": "Model download timeout (10 minutes exceeded)",
                "model_name": model_name
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ Error pulling model {model_name}: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": f"Unexpected error: {str(e)}",
                "model_name": model_name
            }
        )

@app.get("/models/config/status")
async def get_config_status():
    """è·å–é…ç½®æ–‡ä»¶çŠ¶æ€"""
    try:
        config_status = model_manager.get_config_status()
        
        return {
            "success": True,
            "config_status": config_status,
            "message": "Configuration status retrieved successfully"
        }
        
    except Exception as e:
        logger.error(f"Error getting config status: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error getting config status: {str(e)}"}
        )

@app.post("/models/config/save")
async def save_config():
    """æ‰‹åŠ¨ä¿å­˜å½“å‰é…ç½®"""
    try:
        model_manager._save_preferences_to_file()
        config_info = model_manager.get_config_status()
        
        return {
            "success": True,
            "message": "Configuration saved successfully",
            "config_info": config_info
        }
    except Exception as e:
        logger.error(f"Error saving config: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error saving config: {str(e)}"}
        )

@app.post("/models/config/reset")
async def reset_config(request: ConfigResetRequest):
    """é‡ç½®é…ç½®åˆ°é»˜è®¤å€¼"""
    try:
        if not request.confirm:
            return JSONResponse(
                status_code=400,
                content={
                    "error": "Configuration reset requires confirmation",
                    "message": "Set 'confirm' to true to proceed with reset"
                }
            )
        
        success = model_manager.reset_preferences()
        
        if success:
            # é‡æ–°è·å–å¯ç”¨æ¨¡å‹
            await model_manager.get_available_models(force_refresh=True)
            
            return {
                "success": True,
                "message": "Configuration reset successfully",
                "current_model": model_manager.get_current_model(),
                "available_models": model_manager.available_models
            }
        else:
            return JSONResponse(
                status_code=500,
                content={"error": "Failed to reset configuration"}
            )
            
    except Exception as e:
        logger.error(f"Error resetting config: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error resetting config: {str(e)}"}
        )

# Main Processing Endpoints

# 3. ä¿®æ”¹ process_text ç«¯ç‚¹
@app.post("/process_text")
async def process_text(request: TextRequest):
    """Process text input - æ”¯æŒä¸¤ç§æ¨¡å¼ + TTS"""
    try:
        # ä¼ é€’ device_id åˆ°å¤„ç†å‡½æ•°
        result = await process_text_with_enhanced_llm(
            request.text,
            location=request.location or "living_room",
            device_id=request.device_id  # ä¼ é€’ device_id
        )
        return result
        
    except Exception as e:
        logger.error(f"Error processing text: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error processing text: {str(e)}"}
        )

# 4. ä¿®æ”¹ process_audio ç«¯ç‚¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
@app.post("/process_audio")
async def process_audio(request: AudioRequest):
    """Process audio input with TTS support"""
    try:
        audio_path = request.audio_path
        device_id = request.device_id if hasattr(request, 'device_id') else None
        logger.info(f"ğŸ¤ Processing audio: {audio_path}, device_id: {device_id}")
        
        # STT processing
        stt_url = f"http://{STT_HOST}:{STT_PORT}/transcribe"
        stt_response = requests.post(stt_url, json={"audio_path": audio_path})
        
        if stt_response.status_code != 200:
            raise HTTPException(status_code=500, detail="STT service error")
        
        text_result = stt_response.json().get("text", "")
        
        if not text_result:
            return JSONResponse(
                status_code=400,
                content={"error": "Unable to recognize audio content"}
            )
        
        # Process the transcribed text (åŒ…æ‹¬TTS)
        result = await process_text_with_enhanced_llm(
            text_result,
            location="living_room",
            device_id=device_id  # ä¼ é€’ device_id
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Error processing audio: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error processing audio: {str(e)}"}
        )

@app.post("/process_contextual")
async def process_contextual(request: ContextualRequest):
    """Process text with context"""
    try:
        result = await process_text_with_enhanced_llm(
            request.text,
            request.user_context,
            request.location
        )
        return result
        
    except Exception as e:
        logger.error(f"Error processing contextual request: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Error processing contextual request: {str(e)}"}
        )

# Scene and Device Control Endpoints

@app.post("/execute_scene")
async def execute_scene(request: SceneRequest):
    """æ‰§è¡Œåœºæ™¯æ¨¡å¼"""
    try:
        scene_name = request.scene_name
        location = request.location
        
        logger.info(f"ğŸ¬ Executing scene: {scene_name} at {location}")
        
        # è°ƒç”¨ IoT æœåŠ¡æ‰§è¡Œåœºæ™¯
        iot_url = f"http://{IOT_HOST}:{IOT_PORT}/execute_scene"
        iot_response = requests.post(iot_url, json={
            "scene_name": scene_name,
            "location": location
        })
        
        if iot_response.status_code == 200:
            results = iot_response.json()
            
            # å¹¿æ’­åœºæ™¯æ‰§è¡Œäº‹ä»¶åˆ°æ‰€æœ‰ WebSocket å®¢æˆ·ç«¯
            message = {
                "type": "scene_executed",
                "scene": scene_name,
                "location": location,
                "results": results,
                "timestamp": time.time()
            }
            
            for client_id, ws in connected_clients.items():
                try:
                    await ws.send_json(message)
                except:
                    pass
            
            return results
        else:
            raise HTTPException(
                status_code=iot_response.status_code,
                detail="Failed to execute scene"
            )
            
    except Exception as e:
        logger.error(f"Error executing scene: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/scenes")
async def get_available_scenes():
    """è·å–å¯ç”¨çš„åœºæ™¯åˆ—è¡¨"""
    scenes = {
        "sleep_mode": {
            "name": "Sleep Mode",
            "description": "Dim lights, optimal temperature for sleeping",
            "icon": "ğŸŒ™"
        },
        "work_mode": {
            "name": "Work Mode", 
            "description": "Bright lights, focused environment",
            "icon": "ğŸ’¼"
        },
        "movie_mode": {
            "name": "Movie Mode",
            "description": "Ambient lighting, entertainment setup",
            "icon": "ğŸ¬"
        },
        "home_mode": {
            "name": "Home Mode",
            "description": "Welcome home comfort settings",
            "icon": "ğŸ "
        },
        "away_mode": {
            "name": "Away Mode",
            "description": "Energy saving and security",
            "icon": "ğŸš—"
        },
        "cooking_mode": {
            "name": "Cooking Mode",
            "description": "Kitchen optimized settings",
            "icon": "ğŸ‘¨â€ğŸ³"
        }
    }
    return {"scenes": scenes}

@app.get("/devices")
async def get_all_devices():
    """è·å–æ‰€æœ‰è®¾å¤‡çŠ¶æ€"""
    try:
        iot_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/devices")
        if iot_response.status_code == 200:
            return iot_response.json()
        else:
            return {"devices": {}, "error": "IoT service error"}
    except Exception as e:
        logger.error(f"Error getting devices: {str(e)}")
        return {"devices": {}, "error": str(e)}

@app.post("/register_device")
async def register_device(request: DeviceRegistration):
    """æ³¨å†Œæ–°è®¾å¤‡"""
    try:
        device_info = request.device_info
        device_id = device_info.get("device_id", str(uuid.uuid4()))
        
        registered_devices[device_id] = {
            "info": device_info,
            "capabilities": request.capabilities,
            "registered_at": time.time(),
            "last_seen": time.time()
        }
        
        logger.info(f"ğŸ“± Device registered: {device_id}")
        
        # å¹¿æ’­è®¾å¤‡æ³¨å†Œäº‹ä»¶
        for client_id, ws in connected_clients.items():
            try:
                await ws.send_json({
                    "type": "device_registered",
                    "device_id": device_id,
                    "device_info": device_info
                })
            except:
                pass
        
        return {
            "success": True,
            "device_id": device_id,
            "message": "Device registered successfully"
        }
        
    except Exception as e:
        logger.error(f"Error registering device: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Environmental Data Endpoints

@app.get("/environment")
async def get_environment():
    """è·å–ç¯å¢ƒæ•°æ®"""
    try:
        # å°è¯•ä» IoT æœåŠ¡è·å–æœ€æ–°æ•°æ®
        iot_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/sensors")
        if iot_response.status_code == 200:
            return {"sensors": iot_response.json().get("sensors", {})}
    except:
        pass
    
    # è¿”å›æœ¬åœ°æ•°æ®ä½œä¸ºå¤‡ä»½
    return environmental_data

@app.get("/environment/{location}")
async def get_location_environment(location: str):
    """è·å–ç‰¹å®šä½ç½®çš„ç¯å¢ƒæ•°æ®"""
    try:
        # å°è¯•ä» IoT æœåŠ¡è·å–
        iot_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/sensors/{location}")
        if iot_response.status_code == 200:
            return iot_response.json()
    except:
        pass
    
    # ä½¿ç”¨æœ¬åœ°æ•°æ®
    sensors = environmental_data.get("sensors", {})
    if location in sensors:
        return {
            "location": location,
            "data": sensors[location]
        }
    else:
        raise HTTPException(status_code=404, detail=f"Location {location} not found")

# WebSocket for real-time communication

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint - å…¼å®¹å‰ç«¯æœŸæœ›çš„ /ws è·¯å¾„"""
    await websocket.accept()
    client_id = str(id(websocket))  # ç”Ÿæˆå®¢æˆ·ç«¯ID
    connected_clients[client_id] = websocket
    
    logger.info(f"ğŸ”Œ WebSocket client {client_id} connected")
    
    try:
        # å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯
        await websocket.send_json({
            "type": "connection_established",
            "client_id": client_id,
            "current_model": model_manager.get_current_model(),
            "available_models": model_manager.available_models,
            "mode": model_manager.get_current_mode(),
            "timestamp": time.time(),
            "message": "WebSocket connection established successfully"
        })
        
        # å¤„ç†æ¶ˆæ¯å¾ªç¯
        while True:
            # æ¥æ”¶æ–‡æœ¬æ¶ˆæ¯
            data = await websocket.receive_text()
            
            try:
                message = json.loads(data)
                message_type = message.get("type", "")
                
                logger.info(f"ğŸ“¨ Received WebSocket message type: {message_type}")
                
                if message_type == "text":
                    # å¤„ç†æ–‡æœ¬æ¶ˆæ¯
                    text_input = message.get("text", "")
                    user_context = message.get("user_context", {})
                    location = message.get("location", "living_room")
                    
                    # å¤„ç†æ–‡æœ¬ï¼ˆåŒ…æ‹¬TTSï¼‰
                    result = await process_text_with_enhanced_llm(
                        text_input, 
                        user_context, 
                        location
                    )
                    
                    # å‘é€å“åº”
                    await websocket.send_json({
                        "type": "text_response",
                        "response": result,
                        "client_id": client_id,
                        "timestamp": time.time()
                    })
                
                elif message_type == "audio_ready":
                    # éŸ³é¢‘æ–‡ä»¶å¤„ç†
                    audio_path = message.get("audio_path", "")
                    
                    # å‘é€éŸ³é¢‘åˆ° STT æœåŠ¡
                    stt_url = f"http://{STT_HOST}:{STT_PORT}/transcribe"
                    stt_response = requests.post(stt_url, json={"audio_path": audio_path})
                    
                    if stt_response.status_code == 200:
                        transcription = stt_response.json().get("text", "")
                        
                        # å¤„ç†è½¬å½•çš„æ–‡æœ¬
                        result = await process_text_with_enhanced_llm(
                            transcription,
                            message.get("user_context", {}),
                            message.get("location", "living_room")
                        )
                        
                        await websocket.send_json({
                            "type": "audio_response",
                            "transcription": transcription,
                            "response": result,
                            "client_id": client_id
                        })
                    else:
                        await websocket.send_json({
                            "type": "error",
                            "error": "STT service error"
                        })
                
                elif message_type == "get_status":
                    # è·å–å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€
                    
                    # 1. è·å–è®¾å¤‡çŠ¶æ€
                    device_states = {}
                    try:
                        iot_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/devices", timeout=2)
                        if iot_response.status_code == 200:
                            device_states = iot_response.json().get("devices", {})
                    except:
                        logger.warning("Failed to get device states from IoT service")
                    
                    # 2. è·å–ä¼ æ„Ÿå™¨æ•°æ®
                    sensor_data = {}
                    try:
                        sensor_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/sensors", timeout=2)
                        if sensor_response.status_code == 200:
                            sensor_data = sensor_response.json().get("sensors", {})
                    except:
                        sensor_data = environmental_data.get("sensors", {})
                    
                    # 3. æ„å»ºå®Œæ•´çš„çŠ¶æ€å“åº”
                    await websocket.send_json({
                        "type": "status_response",
                        "environmental_data": environmental_data,
                        "device_states": device_states,
                        "sensor_data": sensor_data,
                        "model_info": {
                            "current_model": model_manager.get_current_model(),
                            "available_models": model_manager.available_models,
                            "mode": model_manager.get_current_mode()
                        },
                        "connected_clients": len(connected_clients),
                        "registered_devices": len(registered_devices),
                        "system_uptime": time.time() - startup_time,
                        "timestamp": time.time()
                    })
                
                elif message_type == "execute_scene":
                    # åœºæ™¯æ‰§è¡Œ
                    scene_name = message.get("scene", "")
                    location = message.get("location", None)
                    
                    try:
                        # è°ƒç”¨åœºæ™¯æ‰§è¡Œ
                        scene_response = requests.post(
                            f"http://{IOT_HOST}:{IOT_PORT}/execute_scene",
                            json={"scene_name": scene_name, "location": location}
                        )
                        
                        if scene_response.status_code == 200:
                            results = scene_response.json()
                            await websocket.send_json({
                                "type": "scene_executed",
                                "scene": scene_name,
                                "location": location,
                                "results": results,
                                "timestamp": time.time()
                            })
                        else:
                            await websocket.send_json({
                                "type": "error",
                                "error": f"Failed to execute scene: {scene_name}",
                                "details": scene_response.text
                            })
                    except Exception as e:
                        await websocket.send_json({
                            "type": "error",
                            "error": f"Scene execution error: {str(e)}"
                        })
                
                elif message_type == "device_control":
                    # è®¾å¤‡æ§åˆ¶
                    device = message.get("device", "")
                    action = message.get("action", "")
                    location = message.get("location", "")
                    parameters = message.get("parameters", {})
                    
                    try:
                        # æ‰§è¡Œè®¾å¤‡æ§åˆ¶
                        control_response = requests.post(
                            f"http://{IOT_HOST}:{IOT_PORT}/control",
                            json={
                                "commands": [{
                                    "device": device,
                                    "action": action,
                                    "location": location,
                                    "parameters": parameters
                                }]
                            }
                        )
                        
                        if control_response.status_code == 200:
                            results = control_response.json()
                            await websocket.send_json({
                                "type": "device_control_result",
                                "device": device,
                                "action": action,
                                "location": location,
                                "results": results,
                                "timestamp": time.time()
                            })
                        else:
                            await websocket.send_json({
                                "type": "error",
                                "error": "Device control failed",
                                "details": control_response.text
                            })
                    except Exception as e:
                        await websocket.send_json({
                            "type": "error",
                            "error": f"Device control error: {str(e)}"
                        })
                
                elif message_type == "get_sensors":
                    # è·å–ä¼ æ„Ÿå™¨æ•°æ®
                    location = message.get("location", None)
                    
                    try:
                        if location:
                            sensor_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/sensors/{location}")
                        else:
                            sensor_response = requests.get(f"http://{IOT_HOST}:{IOT_PORT}/sensors")
                        
                        if sensor_response.status_code == 200:
                            sensors = sensor_response.json()
                            await websocket.send_json({
                                "type": "sensor_data",
                                "sensors": sensors,
                                "location": location,
                                "timestamp": time.time()
                            })
                        else:
                            await websocket.send_json({
                                "type": "error",
                                "error": "Failed to get sensor data"
                            })
                    except Exception as e:
                        # å›é€€åˆ°æœ¬åœ°æ•°æ®
                        if location and location in environmental_data.get("sensors", {}):
                            await websocket.send_json({
                                "type": "sensor_data",
                                "sensors": environmental_data["sensors"][location],
                                "location": location,
                                "timestamp": time.time()
                            })
                        else:
                            await websocket.send_json({
                                "type": "sensor_data",
                                "sensors": environmental_data.get("sensors", {}),
                                "location": None,
                                "timestamp": time.time()
                            })
                
                elif message_type == "get_models":
                    # è·å–æ¨¡å‹åˆ—è¡¨
                    models = await model_manager.get_available_models()
                    await websocket.send_json({
                        "type": "models_list",
                        "available_models": models,
                        "current_model": model_manager.get_current_model(),
                        "timestamp": time.time()
                    })
                
                elif message_type == "register_device":
                    # è®¾å¤‡æ³¨å†Œ
                    device_info = message.get("device_info", {})
                    device_id = device_info.get("device_id", str(client_id))
                    
                    registered_devices[device_id] = {
                        "info": device_info,
                        "client_id": client_id,
                        "registered_at": time.time()
                    }
                    
                    await websocket.send_json({
                        "type": "device_registered",
                        "device_id": device_id,
                        "timestamp": time.time()
                    })
                    
                elif message_type == "ping":
                    # å“åº” ping
                    await websocket.send_json({
                        "type": "pong",
                        "timestamp": time.time()
                    })
                    
                else:
                    # æœªçŸ¥æ¶ˆæ¯ç±»å‹
                    await websocket.send_json({
                        "type": "error",
                        "error": f"Unknown message type: {message_type}",
                        "timestamp": time.time()
                    })
                    
            except json.JSONDecodeError:
                await websocket.send_json({
                    "type": "error",
                    "error": "Invalid JSON format",
                    "timestamp": time.time()
                })
            except Exception as e:
                logger.error(f"Error processing WebSocket message: {str(e)}")
                await websocket.send_json({
                    "type": "error",
                    "error": f"Processing error: {str(e)}",
                    "timestamp": time.time()
                })
                
    except WebSocketDisconnect:
        logger.info(f"ğŸ”´ WebSocket client {client_id} disconnected")
    except Exception as e:
        logger.error(f"WebSocket error for client {client_id}: {str(e)}")
    finally:
        # æ¸…ç†æ–­å¼€çš„è¿æ¥
        if client_id in connected_clients:
            del connected_clients[client_id]
        
        # æ¸…ç†æ³¨å†Œçš„è®¾å¤‡
        devices_to_remove = [
            device_id for device_id, device in registered_devices.items()
            if device.get("client_id") == client_id
        ]
        for device_id in devices_to_remove:
            del registered_devices[device_id]
        
        logger.info(f"ğŸ§¹ Cleaned up client {client_id}")

# Helper functions

async def test_model_functionality(model_name: str):
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    try:
        test_prompt = "Hello, this is a test message. Please respond with 'Model test successful' if you can understand this."
        
        if model_manager.current_mode == APIMode.REMOTE_API:
            # API mode test
            payload = {
                "model": model_name,
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": test_prompt}
                ],
                "max_tokens": 50
            }
            
            response = requests.post(
                OLLAMA_ENDPOINT,
                json=payload,
                headers=model_manager.api_headers,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                response_text = data["choices"][0]["message"]["content"].lower()
                
                if "test" in response_text or "successful" in response_text:
                    return {
                        "success": True,
                        "response": data["choices"][0]["message"]["content"],
                        "mode": "api"
                    }
        else:
            # Local mode test
            payload = {
                "model": model_name,
                "prompt": test_prompt,
                "stream": False
            }
            
            response = requests.post(
                f"{OLLAMA_ENDPOINT}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                response_text = data.get("response", "").lower()
                
                if "test" in response_text or "successful" in response_text:
                    return {
                        "success": True,
                        "response": data.get("response", ""),
                        "eval_count": data.get("eval_count", 0),
                        "eval_duration": data.get("eval_duration", 0),
                        "mode": "local"
                    }
        
        return {
            "success": False,
            "error": "Model did not respond as expected"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

async def broadcast_model_switch(new_model: str):
    """å¹¿æ’­æ¨¡å‹åˆ‡æ¢äº‹ä»¶åˆ°æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯"""
    if not connected_clients:
        return
    
    message = {
        "type": "model_switch",
        "new_model": new_model,
        "timestamp": time.time(),
        "message": f"System switched to model: {new_model}"
    }
    
    disconnected_clients = []
    for client_id, websocket in connected_clients.items():
        try:
            await websocket.send_json(message)
        except Exception as e:
            logger.error(f"Failed to broadcast to client {client_id}: {str(e)}")
            disconnected_clients.append(client_id)
    
    # Remove disconnected clients
    for client_id in disconnected_clients:
        del connected_clients[client_id]

async def broadcast_mode_switch(new_mode: str):
    """å¹¿æ’­æ¨¡å¼åˆ‡æ¢äº‹ä»¶åˆ°æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯"""
    if not connected_clients:
        return
    
    message = {
        "type": "mode_switch",
        "new_mode": new_mode,
        "model": model_manager.current_model,
        "timestamp": time.time(),
        "message": f"System switched to {new_mode} mode"
    }
    
    disconnected_clients = []
    for client_id, websocket in connected_clients.items():
        try:
            await websocket.send_json(message)
        except Exception as e:
            logger.error(f"Failed to broadcast mode switch to client {client_id}: {str(e)}")
            disconnected_clients.append(client_id)
    
    # Remove disconnected clients
    for client_id in disconnected_clients:
        del connected_clients[client_id]

# Audio file serving (if needed)
@app.get("/audio/{filename}")
async def serve_audio(filename: str):
    """Serve TTS audio files"""
    try:
        # ä»£ç†åˆ°TTSæœåŠ¡
        tts_audio_url = f"http://{TTS_HOST}:{TTS_PORT}/audio/{filename}"
        
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.get(tts_audio_url)
            
            if response.status_code == 200:
                return Response(
                    content=response.content,
                    media_type=response.headers.get("content-type", "audio/mpeg"),
                    headers={
                        "Content-Disposition": f"inline; filename={filename}",
                        "Cache-Control": "public, max-age=3600"
                    }
                )
            else:
                raise HTTPException(status_code=404, detail="Audio file not found")
                
    except Exception as e:
        logger.error(f"Error serving audio: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Startup event

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global startup_time
    startup_time = time.time()
    
    logger.info("ğŸš€ Starting AI Voice Assistant Coordinator Service")
    logger.info(f"ğŸ“ Running in {model_manager.get_current_mode()} mode")
    logger.info(f"ğŸ¤– Default model: {model_manager.current_model}")
    
    # Check if API key is configured for API mode
    if model_manager.current_mode == APIMode.REMOTE_API and not REMOTE_API_KEY:
        logger.error("âŒ WARNING: API mode selected but REMOTE_API_KEY not configured!")
    
    # Get initial model list
    await model_manager.get_available_models(force_refresh=True)
    
    logger.info("âœ… Coordinator service started successfully")

# Run the application
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)